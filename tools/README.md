PARSEME 2.0 subtask 1 tools
---------------------------

This folder contains python scripts to validate and evaluate files in [CUPT format](https://gitlab.com/parseme/corpora/-/wikis/CUPT-format).
Notice that the official shared task 2.0 evaluation is run in [Codabench](https://www.codabench.org/).
Tools there follow the same input-output conventions as the scripts provided here.

## CUPT Evaluation

The script `parseme_evaluate.py` is used to compare two CUPT files: prediction vs. gold (reference).
It is a slightly refactored copy of the evaluation script from PARSEME shared tasks 1.1 (2018) and 1.2 (2020).
In the past, the script was called `evaluate.py` and relied on a minimal `tsvlib.py` module, now integrated into the script.
We recommend *against* using `tsvlib` to manipulate CUPT; instead, use the [cuptlib](https://gitlab.com/parseme/cuptlib) library.

The documentation below is adapted from the [2018 shared task website](https://multiword.sourceforge.net/PHITE1a4d.html).

### Evaluation metrics

Participants are to provide the output produced by their systems on the (blind) test corpus. 
This output is compared with the gold standard (ground truth). 
Evaluation metrics are precision (P), recall (R) and F1-score (F) of two types: _general metrics_ and _specialized metrics_.

**General metrics** are defined along two dimensions:
  * strict score (per-MWE) vs. fuzzy score (per-token), and 
  * identification scores (disregarding categories) vs. per-category scores
General metrics are described in more detail in Section 6 of the [description paper of PARSEME shared task 1.0](http://aclweb.org/anthology/W17-1704)

**Specialized metrics** are metrics specialized in the following phenomena, providing separate P/R/F scores according to:
  * _Continuity_ - MWEs whose lexicalized components are adjacent vs. non-adjacent
  * _Length_ - multi-token MWEs vs. single-token MWEs
  * _Novelty_ - seen vs. unseen MWEs. A MWE is considered seen if a MWE with the same (multi-)set of lemmas is annotated at least once in the train/dev corpus.
  * _Variability_ - identical occurrences vs. variants (seen MWEs not identical to MWE occurrences from the train/dev corpus)
Specialized metrics are described in more detail in Section 6 of the [description paper of PARSEME shared task 1.1](https://aclanthology.org/W18-4925/)

Scores are calculated both per language and for all participating languages (as a macro-average).
  * F-scores obtained by a system for each language are (arithmetically) averaged.
  * For systems not providing results for a given language, its F-score for this language is considered as equal to 0.
  * If a language has no MWE corresponding to a given phenomenon, we do not include this language it the average.
  
### Evaluation scripts

The evaluation script `parseme_evaluate.py` takes as input two variants of the same test file - the gold standard and the prediction - in CUPT format. 
If you also indicate the training corpus, the script can the calculate novelty- and variability-dedicated metrics described above.

Only columns 1, 2 and 11 are relevant for the global metrics, while columns 1, 2, 3 (lemma) and 11 are relevant for the dedicated metrics. 
All other columns are ignored by the script. The script can be used as follows (`--train` is optional):

`./parseme_evaluate.py --gold gold.cupt --pred system.cupt --train train-dev.cupt`

As per the definition of unseen MWEs, the file given to the option `--train` is not only the training file, but a concatenation of the training and development corpora. 
This file can be easily obtained by a command such as cat `train.cupt dev.cupt > train-dev.cupt` for each language. 

For details of the evaluation, run the script with the `--debug` option, preferably in conjunction with `less`:

`./evaluate.py --gold gold.cupt --pred system.cupt --train train-dev.cupt --debug | less -RS`

In case of errors, the evaluation script will indicate the line of the input file in which the error occurred. 
The error may actually be located in the previous or next sentence, because the script reads sentences one by one before processing them.

In addition to `parseme_evaluate.py`, we also provide a script to macro-average scores across languages, called `average_of_evaluations.py`
This script takes as input several files generated by the evaluation script (you can redirect the output) and generates averages for all metrics. 
For instance, you can run the following commands to calculate the macro-averaged scores between Ancient Greek (GRC) and Polish (PL):
```
cat GRC/train.cupt GRC/dev.cupt > GRC/train-dev.cupt
cat PL/train.cupt PL/dev.cupt > PL/train-dev.cupt
./parseme_evaluate.py --gold GRC/dev.cupt --pred GRC/system.cupt --train GRC/train-dev.cupt > GRC/eval.txt
./parseme_evaluate.py --gold PL/dev.cupt --pred PL/system.cupt --train PL/train-dev.cupt > PL/eval.txt
./average_of_evaluations.py GRC/eval.txt PL/eval.txt
```
The rankings using macro-averaged scores over all languages are calculated using this script.

### Diversity evaluation
We are also working on adding a secondary evaluation dimension dedicated to diversity. We will use 3 diversity measures:
  * richness - the number of correctly identified MWE types
  * Shannon evenness - to estimate how evenly the correctly identified MWE occurrences are in MWE types
  * Shannon-Weaver entropy - a combination of the two above measures
These measures will apply only to the true prositives found by the systems in the test corpora. 

The evaluation scripts for diversity will be published by the end of October 2025.

## CUPT Validation

`parseme_validate.py` is a CUPT validation script.
It can help participants verify their predictions locally before submitting them.
It can be configured thanks to the files in the `valid_parseme` folder.

The validation tests are organized into three levels.
  * Level 1: Test only the CUPT backbone: order of lines, newline encoding, core tests that check the file integrity.
  * Level 2: PARSEME and UD contents.
  * Level 3: PARSEME releases: NotMWE tag excluded, more constraints on metadata.

Validation rules are described in more detail on the [PARSEME corpus annotation website](https://gitlab.com/parseme/corpora/-/wikis/PARSEME-tools#file-format-validation)  
  
A copy of the [Universal Dependencies](https://universaldependencies.org/) validator is provided in `valid_ud` folder.
We embed a UD validator version which is compatible with the UD part (first 10 columns) of PARSEME 2.0 shared task files.
However, notice that the UD validator evolves, so this may not hold true for future versions of the data or of the validator.

The validation script was developed by Van Tuan Bui based on the [UD validator](https://universaldependencies.org/contributing/validation.html) by Filip Ginter, Sampo Pyysalo and Dan Zeman. It is published under GNU-GPL v3 licence.
